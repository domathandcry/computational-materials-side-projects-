{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a1c386-bdc4-45c6-864d-b13c443c5149",
   "metadata": {},
   "source": [
    "-introduction: background of dataset\n",
    "-goal/hypothesis\n",
    "-data preprocessing\n",
    "-visualization\n",
    "-machine learning methods (suggest trying at least 2 different methods)\n",
    "-discussion\n",
    "-concluson (ML result and efficacy as well as intuition from material science and chemistry \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc14c82-baac-4f29-908d-72e79345d40c",
   "metadata": {},
   "source": [
    "## Dataset background and introduction\n",
    "\n",
    "*README*: I made 3 attempts in this final project, where the second one ends up with a much higher $R^2$ value for my regression model and its parity plot. Along the questions, you can see the comments with '#' where I show my first and second attempt. The un-commented one is my final answer.\n",
    "\n",
    "Alloy plays significant roles in structures we see in our daily live has broad applications in  fields that have relation with materials. Properties and performance of alloys significantly depends on its manufacturing methods, and commonly it is done by mixing of different molten metals. In liquid molten state, metal atoms diffuse to spaces between atoms to form various composition and arrangements which then contributes to the alloy's properties. \n",
    "\n",
    "However, while the diffusion coefficients in the equation can be determinedd accurately from the rates of atomic vacancy exchanges around the impurity, these coefficients can only be determined empirically, which is costly and hard to carry out. Only small amount of the coefficients has been experimentally measured. This issue can be optimized by using computational methods.\n",
    "\n",
    "This dataset is a product of a computational method that could automate generation of dilute solute diffusion systems in Mg, Al, Cu, Ni, Pd and Pt host lattices using high-throughput DFT calculations. Parameters calculated are fitted into a multi-frequency framework developed previously, expressed with Arrhenius equation:\n",
    "\n",
    "$\\omega_{i}$ = $\\nu_{i}$ $\\exp$ $({ \\frac{-E_{i}}{k_{b}{T}} })$\n",
    "\n",
    "where $\\omega_{i}$ is jump frequency, $-E_{i}$ is migration barrier energy, $\\nu_{i}$ is attempt frequencies.\n",
    "\n",
    "In order to calculate solute diffusion coefficients in HCP and FCC hosts, the 8-frequency diffusion model and the 5-frequency diffusion model were used, corresponding to the 2 hosts.<br>\n",
    "\n",
    "The article that describes the computational method that generated this data can be found here: https://www.nature.com/articles/sdata201654#Tab1 <br>\n",
    "\n",
    "Data downloaded from here: https://figshare.com/articles/dataset/MAST-ML_Education_Datasets/7017254<br>\n",
    "\n",
    "The material property this dataset seeks to predict is the effective diffusion activation energy for vacancy-mediated diffusion of a dilute solute element in a known host crystal from the migration barrier activation energy. The reported energies have all been normalized to the host value, such that each energy in a host-solute pair is the relative energy to the respective host's self-diffusion value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa8a522-7659-41d3-a347-62b96f4a3270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kneed\n",
      "  Using cached kneed-0.8.5-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: numpy>=1.14.2 in /opt/conda/lib/python3.11/site-packages (from kneed) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from kneed) (1.13.0)\n",
      "Using cached kneed-0.8.5-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: kneed\n",
      "Successfully installed kneed-0.8.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting tensorflow-cpu==2.17.0\n",
      "  Using cached tensorflow_cpu-2.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (1.63.0)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow-cpu==2.17.0)\n",
      "  Using cached tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.2.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (0.37.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.11/site-packages (from tensorflow-cpu==2.17.0) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-cpu==2.17.0) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow-cpu==2.17.0) (13.9.4)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow-cpu==2.17.0) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow-cpu==2.17.0) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow-cpu==2.17.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow-cpu==2.17.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow-cpu==2.17.0) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow-cpu==2.17.0) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow-cpu==2.17.0) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow-cpu==2.17.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow-cpu==2.17.0) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-cpu==2.17.0) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow-cpu==2.17.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow-cpu==2.17.0) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-cpu==2.17.0) (0.1.2)\n",
      "Using cached tensorflow_cpu-2.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (221.2 MB)\n",
      "Using cached tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
      "Installing collected packages: tensorboard, tensorflow-cpu\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.16.2\n",
      "    Uninstalling tensorboard-2.16.2:\n",
      "      Successfully uninstalled tensorboard-2.16.2\n",
      "  Attempting uninstall: tensorflow-cpu\n",
      "    Found existing installation: tensorflow-cpu 2.16.1\n",
      "    Uninstalling tensorflow-cpu-2.16.1:\n"
     ]
    }
   ],
   "source": [
    "!pip install kneed\n",
    "!pip install tensorflow-cpu==2.17.0\n",
    "!pip install scikeras\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd165e-65e3-48cb-a973-ffb7594c2cf2",
   "metadata": {},
   "source": [
    "## Goal/hypothesis\n",
    "\n",
    "Since the dataset is used to predict the migration activation barrier energy (Enorm, as in normalized migration activation energy barrier), I will need to identify features that help predict the normalized migration activation energy barrier. <br>\n",
    "\n",
    "Hypothesis: Intuitively, I think any feature that relates to bonding strength/a proxy related to bonding strength can be used as a prediction feature, since how the atoms bond together significantly affects the amount of energy needed to take them apart. These properties can be used to create a model that predicts how much is the migration activation energy barrier. <br>\n",
    "\n",
    "The higher the melting temperature, the higher the migration activation energy barrier; \n",
    "the more unfilled d-orbitals lead to more directional bonding, thus a higher activation energy to break the bonds. \n",
    "Besides that, the lower stability of the solute in BCC correlates with weaker bonding in the host lattice, which means a lower activation energy barrier. While radii and charges (electronegativity) of the atom is also important in bonding, these features shall also be included.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9367fe-5a10-406a-9bba-aaefcb19706c",
   "metadata": {},
   "source": [
    "## Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2e5efb-ae59-49e3-a72b-da02d60df708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the dataset\n",
    "df = pd.read_csv('Dilute_Solute_Diffusion_with_features.csv')\n",
    "\n",
    "#check how many samples we have\n",
    "print('Shape of data',df.shape)\n",
    "\n",
    "#check keys \n",
    "print('\\nKeys of the data:',df.keys())\n",
    "\n",
    "#check NA values\n",
    "df.isna().sum() #no NaN values, amazing!\n",
    "\n",
    "#'Material compositions 1' and 'Material compositions 2' are string. let's map them to integer\n",
    "uniq_mat = pd.unique(df[['Material compositions 1','Material compositions 2']].values.ravel())\n",
    "str2int = {mat: i+1 for i, mat in enumerate(uniq_mat)}\n",
    "df['mat1_mapped'] = df['Material compositions 1'].map(str2int)\n",
    "df['mat2_mapped'] = df['Material compositions 2'].map(str2int) \n",
    "\n",
    "pop1 = df.pop('mat1_mapped')\n",
    "pop2 = df.pop('mat2_mapped')\n",
    "\n",
    "df.insert(2, 'mat1_mapped', pop1)\n",
    "df.insert(3, 'mat2_mapped', pop2)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa92ade-ffcb-4e1f-a5de-9545026241a5",
   "metadata": {},
   "source": [
    "## Visualization \n",
    "\n",
    "I start by checking the correlation between the features to narrow down which to pick. Correlation is purely statistical; it can have no meaning at all (explanation in the discussion before machine learning methods). Then I check their pairwise $R^2$ value while mapped to Enorm. After that, I also check the collinearity between the features. Lastly, I combined all possible variable pairs together and checked their $R^2$ values while mapped to Enorm. \n",
    "\n",
    "This step helps visualize the structure and check which feature to extract to train a model that could predict the migration energy barrier based on given properties. I can do a visualization to check the trend between the keys in both 3D and 2D plots against Enorm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e747f2-a348-475a-ac4f-a4852d311100",
   "metadata": {},
   "source": [
    "Let's check for correlation between the features and have a general idea of the structure of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a602d9-c4de-4902-8eae-57551d8708d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df = df.iloc[:,2:]\n",
    "cor = map_df.corr()\n",
    "plt.figure(figsize=(40, 40)) \n",
    "sns.heatmap(cor, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Pearson Correlation Heatmap Pre-filtered')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a6280b-947a-4662-9e2b-55d0d0e3f107",
   "metadata": {},
   "source": [
    "The heatmap shows the following keys are highly correlated to Enorm:\n",
    "\n",
    "Site2_MeltingT - melting temperature of solute<br>\n",
    "Site2_BCCenergy_pa - per-atom DFT energy of the solute placed in a BCC structure, even if the solute is not BCC in reality.<br>\n",
    "Site2_Density - density of solute<br>\n",
    "Site2_NdUnfilled - number of unfilled d-electrons (d-band vacancies) of the solute element.<br>\n",
    "Site2_Group - group of solute <br>\n",
    "\n",
    "Let's check their<br> \n",
    "1.) pairwise $R^2$ values mapped to Enorm, <br>\n",
    "2.) combined possible variable pairs and their $R^2$ values while mapped to Enorm. <br>\n",
    "3.) collinearity between the features themselves,<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc89640-0023-49b8-bd23-5969ddcb6661",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import linregress\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "df = map_df.copy()\n",
    "target = 'Enorm (eV)'\n",
    "\n",
    "all_columns = [\n",
    "    'Material compositions 1', 'Material compositions 2', 'Enorm (eV)', 'E_raw (eV)',\n",
    "    'Site2_MeltingT', 'Site1_MendeleevNumber', 'Site1_MiracleRadius',\n",
    "    'GSestFCClatcnt_max_value', 'Site2_BCCenergy_pa', 'Site1_BCCfermi',\n",
    "    'CovalentRadius_max_value', 'Site2_Density', 'n_ws^third_min_value',\n",
    "    'Site1_HHIr', 'IonicRadii_max_value', 'BoilingT_max_value',\n",
    "    'valence_arithmetic_average', 'Site2_NdUnfilled', 'Site1_IonicRadii',\n",
    "    'BCCenergy_pa_composition_average', 'MiracleRadius_min_value',\n",
    "    'MeltingT_min_value', 'NUnfilled_max_value', 'Site2_Group',\n",
    "    'Site1_CovalentRadii', 'Site2_NUnfilled', 'SpecificHeatCapacity_difference',\n",
    "    'Site1_Electronegativity', 'BCCenergy_pa_arithmetic_average'\n",
    "]\n",
    "\n",
    "features = []\n",
    "for col in all_columns:\n",
    "    if col in df.columns and col != target and pd.api.types.is_numeric_dtype(df[col]):\n",
    "        if df[col].nunique() > 1:\n",
    "            features.append(col)\n",
    "\n",
    "print(f\"Using {len(features)} valid numeric features.\\n\")\n",
    "\n",
    "\n",
    "################### single feature r^2 plot\n",
    "\n",
    "r2_single = []\n",
    "for feat in features:\n",
    "    data = df[[feat, target]].dropna()\n",
    "    if len(data) >= 3:\n",
    "        r2 = linregress(data[feat], data[target]).rvalue ** 2\n",
    "        r2_single.append({'Feature': feat, 'R²': round(r2, 5)})\n",
    "\n",
    "df_single = pd.DataFrame(r2_single).sort_values('R²', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, max(9, len(df_single)*0.42)))\n",
    "y_pos = np.arange(len(df_single))\n",
    "bars = ax.barh(y_pos, df_single['R²'], color='#1976d2', alpha=0.9, height=0.8)\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(df_single['Feature'], fontsize=10.5)\n",
    "ax.set_xlabel('R² Value', fontsize=14)\n",
    "ax.set_title('Single-Feature Linear Correlation with\\nNormalized Migration Activation Energy Barrier',\n",
    "              fontsize=17, fontweight='bold', pad=25)\n",
    "ax.set_xlim(0, 1.05)\n",
    "ax.grid(True, axis='x', alpha=0.3, linestyle='--')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "for i, val in enumerate(df_single['R²']):\n",
    "    ax.text(val + 0.008, i, f'{val:.5f}', va='center', fontsize=10, fontweight='bold', color='#d32f2f')\n",
    "\n",
    "plt.subplots_adjust(left=0.42, right=0.98, top=0.90, bottom=0.12)\n",
    "plt.show()\n",
    "\n",
    "print(\"Single feature R^2 ranking mapped to Enorm\".center(80))\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', '{:.5f}'.format)\n",
    "display(df_single.style\n",
    "        .bar(subset=['R²'], color='#90caf9')\n",
    "        .set_properties(**{'font-size': '12pt', 'font-family': 'Calibri'})\n",
    "        .set_table_styles([{'selector': 'th', 'props': [('font-size', '13pt')]}])\n",
    "        .format({'R²': '{:.5f}'})\n",
    "        .background_gradient(subset=['R²'], cmap='Blues'))\n",
    "\n",
    "################### pair r^2 plot\n",
    "\n",
    "r2_pairs = []\n",
    "for i, (f1, f2) in enumerate(combinations(features, 2)):\n",
    "    if i % 100 == 0 and i > 0:\n",
    "        print(f\"   → {i} pairs processed...\")\n",
    "    data = df[[f1, f2, target]].dropna()\n",
    "    if len(data) < 5:\n",
    "        continue\n",
    "    X = np.column_stack((data[f1], data[f2], np.ones(len(data))))\n",
    "    y = data[target]\n",
    "    try:\n",
    "        coeffs = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "        y_pred = X @ coeffs\n",
    "        r2 = 1 - np.sum((y - y_pred)**2) / np.sum((y - y.mean())**2)\n",
    "        if r2 > 0:\n",
    "            r2_pairs.append({'Feature 1': f1, 'Feature 2': f2, 'R²': round(r2, 5)})\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "df_pairs = pd.DataFrame(r2_pairs).sort_values('R²', ascending=False).reset_index(drop=True)\n",
    "print(f\"\\nFinished! Found {len(df_pairs)} valid pairwise combinations.\\n\")\n",
    "\n",
    "# Heatmap (top 18 features)\n",
    "top_n = min(18, len(df_single))\n",
    "top_features = df_single.head(top_n)['Feature'].tolist()\n",
    "n = len(top_features)\n",
    "matrix = np.zeros((n, n))\n",
    "for _, row in df_pairs.iterrows():\n",
    "    if row['Feature 1'] in top_features and row['Feature 2'] in top_features:\n",
    "        i = top_features.index(row['Feature 1'])\n",
    "        j = top_features.index(row['Feature 2'])\n",
    "        matrix[i, j] = matrix[j, i] = row['R²']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 14))\n",
    "im = ax.imshow(matrix, cmap='magma', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(n))\n",
    "ax.set_yticks(range(n))\n",
    "ax.set_xticklabels(top_features, rotation=90, fontsize=10)\n",
    "ax.set_yticklabels(top_features, fontsize=10)\n",
    "ax.set_title(f'Pairwise R² Heatmap — Top {top_n} Single Features\\nvs Normalized Migration Activation Energy Barrier',\n",
    "             fontsize=18, fontweight='bold', pad=20)\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "cbar.set_label('R² (Two-Feature Model)', fontsize=14, labelpad=15)\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        if matrix[i,j] > 0.8:\n",
    "            ax.text(j, i, f'{matrix[i,j]:.3f}', ha='center', va='center',\n",
    "                    color='white', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"highest 30 feature pairs\".center(100))\n",
    "top_30 = df_pairs.head(30).copy()\n",
    "top_30.index = range(1, len(top_30)+1)\n",
    "\n",
    "display(top_30.style\n",
    "        .bar(subset=['R²'], color='#ff9999')\n",
    "        .set_properties(**{'font-size': '13pt', 'font-family': 'Arial'})\n",
    "        .background_gradient(subset=['R²'], cmap='Reds')\n",
    "        .format({'R²': '{:.5f}'})\n",
    "        .set_table_styles([\n",
    "            {'selector': 'th', 'props': [('font-size', '14pt'), ('background-color', '#ffcdd2')]},\n",
    "            {'selector': 'td', 'props': [('padding', '10px')]}\n",
    "        ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8341fd32-48c2-47c9-83ef-9a5083662aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collinearity plot\n",
    "df = map_df.copy()\n",
    "\n",
    "features = [\n",
    "    'E_raw (eV)', 'Site2_MeltingT', 'Site1_MendeleevNumber', 'Site1_MiracleRadius',\n",
    "    'GSestFCClatcnt_max_value', 'Site2_BCCenergy_pa', 'Site1_BCCfermi',\n",
    "    'CovalentRadius_max_value', 'Site2_Density', 'n_ws^third_min_value',\n",
    "    'Site1_HHIr', 'IonicRadii_max_value', 'BoilingT_max_value',\n",
    "    'valence_arithmetic_average', 'Site2_NdUnfilled', 'Site1_IonicRadii',\n",
    "    'BCCenergy_pa_composition_average', 'MiracleRadius_min_value',\n",
    "    'MeltingT_min_value', 'NUnfilled_max_value', 'Site2_Group',\n",
    "    'Site1_CovalentRadii', 'Site2_NUnfilled', 'SpecificHeatCapacity_difference',\n",
    "    'Site1_Electronegativity', 'BCCenergy_pa_arithmetic_average'\n",
    "]\n",
    "\n",
    "valid_features = [f for f in features if f in df.columns and pd.api.types.is_numeric_dtype(df[f]) and df[f].nunique() > 1]\n",
    "\n",
    "num_df = df[valid_features]\n",
    "\n",
    "corr = num_df.corr(method='pearson')\n",
    "r2_matrix = corr ** 2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 16))\n",
    "sns.heatmap(r2_matrix, annot=True, fmt='.2f', cmap='coolwarm', vmin=0, vmax=1,\n",
    "            linewidths=0.5, linecolor='white', cbar_kws={'shrink': 0.8, 'label': 'R² (Collinearity)'})\n",
    "ax.set_title('Pairwise R² Collinearity Between Features', fontsize=18, fontweight='bold', pad=20)\n",
    "ax.tick_params(axis='both', labelsize=11)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "pairs = []\n",
    "for i in range(len(valid_features)):\n",
    "    for j in range(i+1, len(valid_features)):\n",
    "        r2 = r2_matrix.iloc[i, j]\n",
    "        pairs.append({'Feature 1': valid_features[i], 'Feature 2': valid_features[j], 'R²': round(r2, 4)})\n",
    "\n",
    "df_collinear = pd.DataFrame(pairs).sort_values('R²', ascending=False).reset_index(drop=True)\n",
    "\n",
    "display(df_collinear.head(30).style\n",
    "        .bar(subset=['R²'], color='#e57373')\n",
    "        .background_gradient(subset=['R²'], cmap='Reds')\n",
    "        .format({'R²': '{:.4f}'})\n",
    "        .set_properties(**{'font-size': '13pt', 'text-align': 'left'})\n",
    "        .set_table_styles([{'selector': 'th', 'props': [('font-size', '14pt'), ('background-color', '#ffcdd2')]}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241b1f5f-b0b9-4690-be24-ad454422c4c3",
   "metadata": {},
   "source": [
    "Let's do some 3D and 2D $R^2$ visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d756b7d7-b335-4d92-9658-fb629702039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import linregress, t\n",
    "\n",
    "df = map_df.copy()         \n",
    "target = 'Enorm (eV)'\n",
    "\n",
    "# Make sure df_single exists and has clean column name\n",
    "df_single = df_single.copy()\n",
    "if 'R²' in df_single.columns:\n",
    "    df_single = df_single.rename(columns={'R²': 'R2'})\n",
    "\n",
    "top_4_features = df_single.head(4).copy()\n",
    "\n",
    "pretty_names = {\n",
    "    'Site2_MeltingT': 'Site 2 Melting T',\n",
    "    'Site2_BCCenergy_pa': 'Site 2 BCC Energy',\n",
    "    'Site2_Density': 'Site 2 Density',\n",
    "    'Site2_NdUnfilled': 'Site 2 Nd Unfilled',\n",
    "    'Site2_Group': 'Site 2 Group',\n",
    "    'E_raw (eV)': 'Raw Energy',\n",
    "    'Site1_MendeleevNumber': 'Site 1 Mendeleev Number',\n",
    "    'CovalentRadius_max_value': 'Max Covalent Radius',\n",
    "    'IonicRadii_max_value': 'Max Ionic Radius',\n",
    "    'BoilingT_max_value': 'Max Boiling T',\n",
    "    'MeltingT_min_value': 'Min Melting T',\n",
    "    'valence_arithmetic_average': 'Valence (avg)',\n",
    "    'BCCenergy_pa_composition_average': 'BCC Energy (avg)',\n",
    "    'BCCenergy_pa_arithmetic_average': 'BCC Energy (arith avg)',\n",
    "    'Site1_Electronegativity': 'Site 1 Electronegativity',\n",
    "    'SpecificHeatCapacity_difference': 'ΔSpecific Heat',\n",
    "}\n",
    "\n",
    "colors = ['#d62728', '#ff7f0e', '#2ca02c', '#1f77b4']\n",
    "markers = ['o', 's', '^', 'D']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, row in top_4_features.iterrows():\n",
    "    feat = row['Feature']\n",
    "    r2 = row['R2']\n",
    "\n",
    "    ax = axes[idx]\n",
    "    x = df[feat].values\n",
    "    y = df[target].values\n",
    "\n",
    "    result = linregress(x, y)\n",
    "    slope, intercept = result.slope, result.intercept\n",
    "\n",
    "    sc = ax.scatter(x, y, color=colors[idx], edgecolor='k', s=100,\n",
    "                    marker=markers[idx], alpha=0.9, zorder=5,\n",
    "                    label=pretty_names.get(feat, feat))\n",
    "\n",
    "    line_x = np.linspace(x.min(), x.max(), 100)\n",
    "    line_y = slope * line_x + intercept\n",
    "    ax.plot(line_x, line_y, color=colors[idx], lw=2.5, zorder=6)\n",
    "\n",
    "    n = len(x)\n",
    "    se = result.stderr * np.sqrt(1/n + (line_x - x.mean())**2 / ((n-1)*np.var(x)))\n",
    "    t_val = t.ppf(0.975, n-2)\n",
    "    margin = t_val * se\n",
    "    ax.fill_between(line_x, line_y - margin, line_y + margin,\n",
    "                    color=colors[idx], alpha=0.15, zorder=4)\n",
    "\n",
    "    ax.set_xlabel(pretty_names.get(feat, feat), fontsize=14)\n",
    "    ax.set_title(f'{pretty_names.get(feat, feat)}\\n(R² = {r2:.5f})', fontsize=15, pad=15)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "    # R² box\n",
    "    ax.text(0.05, 0.90, f'R² = {r2:.5f}', transform=ax.transAxes,\n",
    "            fontsize=14, fontweight='bold', color=colors[idx],\n",
    "            bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"white\", alpha=0.9, edgecolor=colors[idx]))\n",
    "\n",
    "#shared y\n",
    "fig.text(0.02, 0.5, 'Normalized Migration Activation Energy Barrier (eV)',\n",
    "         va='center', ha='center', rotation='vertical', fontsize=16)\n",
    "\n",
    "plt.suptitle('Top 4 Single Features vs Normalized Migration Activation Energy Barrier',\n",
    "             fontsize=20, fontweight='bold', y=0.95)\n",
    "\n",
    "handles = [plt.Line2D([0], [0], marker=m, color='w', markerfacecolor=c, markersize=12, markeredgecolor='k')\n",
    "           for m, c in zip(markers, colors)]\n",
    "labels = [pretty_names.get(feat, feat) for feat in top_4_features['Feature']]\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.90),\n",
    "           ncol=2, fontsize=14, frameon=False)\n",
    "\n",
    "plt.subplots_adjust(left=0.10, right=0.98, top=0.84, bottom=0.10, wspace=0.25, hspace=0.35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280a89b-c87d-4b51-9c67-c11f970618ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "df = map_df.copy()          \n",
    "target = 'Enorm (eV)'\n",
    "\n",
    "\n",
    "df_pairs = df_pairs.copy()\n",
    "\n",
    "\n",
    "df_pairs.columns = [col.replace('²', '2').replace(' ', '_') for col in df_pairs.columns]\n",
    "\n",
    "\n",
    "if 'Feature_1' not in df_pairs.columns:\n",
    "    df_pairs = df_pairs.rename(columns={'Feature 1': 'Feature_1', 'Feature 2': 'Feature_2', 'R2': 'R2', 'R²': 'R2'})\n",
    "\n",
    "\n",
    "top_4 = df_pairs.head(4).copy()\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 18))\n",
    "fig.suptitle('Top 4 Strongest Feature Pairs vs Normalized Migration Activation Energy Barrier',\n",
    "             fontsize=24, fontweight='bold', y=0.95)\n",
    "\n",
    "for idx, (_, row) in enumerate(top_4.iterrows(), 1):\n",
    "    f1 = row['Feature_1']\n",
    "    f2 = row['Feature_2']\n",
    "    r2 = row['R2']\n",
    "\n",
    "    ax = fig.add_subplot(2, 2, idx, projection='3d')\n",
    "\n",
    "    x = df[f1].values\n",
    "    y = df[f2].values\n",
    "    z = df[target].values\n",
    "\n",
    "\n",
    "    sc = ax.scatter(x, y, z, c=z, cmap='viridis', s=100, edgecolor='k', alpha=0.9, depthshade=False)\n",
    "\n",
    "\n",
    "    X_fit = np.column_stack((x, y, np.ones(len(x))))\n",
    "    coeffs = np.linalg.lstsq(X_fit, z, rcond=None)[0]\n",
    "    a, b, c = coeffs\n",
    "\n",
    "    x_surf, y_surf = np.meshgrid(np.linspace(x.min(), x.max(), 10),\n",
    "                                 np.linspace(y.min(), y.max(), 10))\n",
    "    z_surf = a * x_surf + b * y_surf + c\n",
    "    ax.plot_surface(x_surf, y_surf, z_surf, color='red', alpha=0.3, linewidth=0)\n",
    "\n",
    "\n",
    "    name1 = pretty_names.get(f1, f1)\n",
    "    name2 = pretty_names.get(f2, f2)\n",
    "    ax.set_xlabel(name1, fontsize=13, labelpad=12)\n",
    "    ax.set_ylabel(name2, fontsize=13, labelpad=12)\n",
    "    ax.set_zlabel('Enorm (eV)', fontsize=13, labelpad=12)\n",
    "    ax.set_title(f'{name1}\\nvs {name2}\\n(R² = {r2:.5f})', fontsize=14, pad=20)\n",
    "\n",
    "\n",
    "    ax.text2D(0.05, 0.88, f'R² = {r2:.5f}', transform=ax.transAxes,\n",
    "              fontsize=16, fontweight='bold', color='red',\n",
    "              bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"white\", edgecolor=\"red\", alpha=0.9))\n",
    "\n",
    "    ax.view_init(elev=20, azim=40 + idx*25)\n",
    "\n",
    "\n",
    "cbar_ax = fig.add_axes([0.92, 0.25, 0.02, 0.5])\n",
    "cbar = fig.colorbar(sc, cax=cbar_ax)\n",
    "cbar.set_label('Normalized Migration Activation Energy Barrier (eV)', fontsize=14, labelpad=15)\n",
    "\n",
    "plt.subplots_adjust(left=0.05, right=0.90, top=0.88, bottom=0.08, wspace=0.1, hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82983a0d-d69a-4e7c-861a-c32e253cb3f6",
   "metadata": {},
   "source": [
    "Discussion: \n",
    "\n",
    "Ran a few metrics to determine our candidate features. \n",
    "\n",
    "Since correlation is purely statistical, it can have no meaning (say, there are 2 variables, ice-cream sold at the HUB during finals vs frequency of rain in Seattle, they can have a high positive or negative relation, but they are actually irrelevant). Therefore, I tried other metrics to seek more clues. I did linear regression on all features to Enorm and see how much variance in Enorm can be explained by the feature using $R^2$. <br>\n",
    "\n",
    "A high value of $R^2$ means the feature maps well to Enorm (change of feature changes Enorm predictively). I also ran a brute force test to skim the $R^2$ values with different combinations of features to shed light on any feature interaction and also collinearity between features. Paired $R^2$ can discover relation that are weak when single, and collinearity between features can get us some clue regarding how features relates to each other, but further domain knowledge intepretation is needed.<br> \n",
    "\n",
    "For example, $R^2$ collinearity between 'BCCenergy_pa_composition_average' and 'BCCenergy_pa_arithmetic_average' is 1, and it is because they are derived from each other mathematically and would not give significant insights, which shows that we cannot simply just take features with the highest $R^2$ values from different metrics. \n",
    "\n",
    "There are a few features that consistently show up in the different top $R^2$ rankings, or has high $R^2$ values when paired with another feature and had features interaction. These features are: \n",
    "\n",
    "1.) Site2_MeltingT <br>\n",
    "2.) Site2_BCCenergy_pa (and how it expressed arithmetically), <br>\n",
    "3.) Site2_NdUnfilled <br>\n",
    "4.) Site2_Density<br>\n",
    "\n",
    "These 4 features are the highest single $R^2$ score with Enorm. Site2_BCCenergy (also in different form like bccenergy_arith/composition) and the other 3 features also has relatively high paired $R^2$ score compared with other features with lower score. Fundamentally, the evaluations provide different insights even though they all end up with a $R^2$ score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31fb90-ec4a-4f53-a314-99b61e1279de",
   "metadata": {},
   "source": [
    "## Machine learning methods to apply\n",
    "\n",
    "Now I picked 4 dimensions to work on. <br>\n",
    "\n",
    "I am aware that the dataset is about predicting continuous variable and not categorical variable, and is a supervised learning issue since I have labelled data. The method that I am going to use is random forest and neural network. But before that, I can try implementing PCA onto my dataset. (although its not needed since my dataset is not too huge or have too much dimension i.e. wouldn't have to worry about curse of dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42131bb6-1a0d-4cde-a970-4a324593c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# # first attempt\n",
    "# filtered_df = map_df[['Site2_MeltingT',\n",
    "#                       'Site2_BCCenergy_pa', 'Site2_NdUnfilled',\n",
    "#                       'Site2_Density']].copy()\n",
    "\n",
    "# #second attempt \n",
    "# filtered_df = map_df[['Site2_MeltingT', 'Site2_BCCenergy_pa', 'Site2_NdUnfilled', \n",
    "#                    'Site2_Density', 'Site1_MendeleevNumber', 'valence_arithmetic_average',\n",
    "#                    'Site1_Electronegativity','Site1_MendeleevNumber','Site1_CovalentRadii','Site1_MiracleRadius', 'BCCenergy_pa_arithmetic_average']].copy()\n",
    "\n",
    "#third attempt\n",
    "filtered_df = map_df[['Site2_MeltingT', 'Site2_BCCenergy_pa', 'Site2_NdUnfilled', \n",
    "                   'Site2_Density', 'Site1_MendeleevNumber', 'valence_arithmetic_average', 'IonicRadii_max_value',\n",
    "                   'Site1_Electronegativity','Site1_MendeleevNumber','Site1_CovalentRadii','Site1_MiracleRadius', 'BCCenergy_pa_arithmetic_average']].copy()\n",
    "\n",
    "explained_variance_ratio_sum = []\n",
    "n_components_range = range(1, len(filtered_df.columns) + 1)\n",
    "\n",
    "for n in n_components_range:\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(filtered_df)\n",
    "    explained_variance_ratio_sum.append(pca.explained_variance_ratio_.sum())\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.style.use('default')\n",
    "\n",
    "plt.plot(n_components_range, explained_variance_ratio_sum,\n",
    "         marker='o', linewidth=3, markersize=10,\n",
    "         color='#1f77b4', markerfacecolor='white', markeredgewidth=2)\n",
    "\n",
    "plt.title('Cumulative Explained Variance by PCA Components',\n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Number of Principal Components', fontsize=14)\n",
    "plt.ylabel('Cumulative Explained Variance Ratio', fontsize=14)\n",
    "plt.xticks(n_components_range)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "for i, var in zip(n_components_range, explained_variance_ratio_sum):\n",
    "    plt.text(i, var + 0.03, f'{var:.4f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Cumulative Explained Variance:\")\n",
    "for n, var in zip(n_components_range, explained_variance_ratio_sum):\n",
    "    print(f\"  {n} component(s) → {var:.4f} ({var*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b60cd3d-639d-40ef-bffe-de63bc704df9",
   "metadata": {},
   "source": [
    "It seems that even with 6 components, $94.43%$ of the total explained variance ratio can be achieved. In other words, if there is a necessity to reduce computational cost, 6 components can be enough. <br>\n",
    "\n",
    "Let's try to build a neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec34cc-1139-4bd4-b8fa-3366cde6fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "import scikeras\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # these features gave 0.435 R^2 on parity plot for NN\n",
    "# filtered_df = map_df[['Site2_MeltingT',\n",
    "#                       'Site2_BCCenergy_pa', 'Site2_NdUnfilled',\n",
    "#                       'Site2_Density']].copy().astype(float)\n",
    "\n",
    "\n",
    "# #these features gave 0.76 R^2\n",
    "# filtered_df = map_df[['Site2_MeltingT', 'Site2_BCCenergy_pa', 'Site2_NdUnfilled', \n",
    "#                    'Site2_Density', 'Site1_MendeleevNumber', 'valence_arithmetic_average', \n",
    "#                    'Site1_Electronegativity','Site1_MendeleevNumber','Site1_CovalentRadii','Site1_MiracleRadius', 'BCCenergy_pa_arithmetic_average']].copy().astype(float)\n",
    "\n",
    "\n",
    "#even more features, gave 0.809 R^2 !\n",
    "filtered_df = map_df[['Site2_MeltingT', 'Site2_BCCenergy_pa', 'Site2_NdUnfilled', \n",
    "                   'Site2_Density', 'Site1_MendeleevNumber', 'valence_arithmetic_average', 'IonicRadii_max_value',\n",
    "                   'Site1_Electronegativity','Site1_MendeleevNumber','Site1_CovalentRadii','Site1_MiracleRadius', 'BCCenergy_pa_arithmetic_average']].copy().astype(float)\n",
    "\n",
    "target = map_df[['Enorm (eV)']].astype(float)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(filtered_df, target,\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce246c-df76-4c4a-9218-e55e58e14842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning\n",
    "\n",
    "def nn1(neurons1=64,neurons2=64, neurons3= 32, activation1='relu',activation2='relu', activation3='linear'):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Input(shape=(x_train.shape[1],)) )  # Using Input layer\n",
    "    \n",
    "    # activation1_instance = activation1\n",
    "    \n",
    "    model.add(Dense(neurons1, kernel_initializer='normal', activation=activation1))\n",
    "    \n",
    "    # activation2_instance = activation2\n",
    "    \n",
    "    model.add(Dense(neurons2, kernel_initializer='normal', activation=activation2))\n",
    "\n",
    "    model.add(Dense(neurons3, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "     # activation3_instance = activation3\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation=activation3))\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate =0.0001)\n",
    "        \n",
    "    model.compile(loss='huber', optimizer=optimizer, metrics=['mae', 'RootMeanSquaredError'])    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72f4a44-2b3b-43e5-a262-03479a87f112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regressor_search1 = KerasRegressor(\n",
    "    model=nn1,\n",
    "    neurons1=64,\n",
    "    neurons2=64,\n",
    "    neurons3=32,\n",
    "    activation1='relu',\n",
    "    activation2='relu',\n",
    "    activation3='relu',  # <-- Add this default\n",
    "    batch_size=32,\n",
    "    epochs=100,  # Default epochs\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(regressor_search1.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51c2fc7-dd95-48b0-8021-a8cfe2593914",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters1 = {\n",
    "    \"neurons1\": np.arange(16, 65, 16).tolist(),  \n",
    "    \"neurons2\": np.arange(16, 65, 16).tolist(),\n",
    "    \"neurons3\": np.arange(16, 33, 8).tolist(),   \n",
    "    \"activation1\": ['tanh', 'relu'],\n",
    "    \"activation2\": ['tanh', 'relu'],\n",
    "    \"activation3\": ['tanh', 'relu','linear'],            \n",
    "    \"batch_size\": np.arange(8, 65, 8).tolist(), \n",
    "    \"epochs\": [50, 100, 150],                    \n",
    "    \"verbose\": [0]                               \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4928a5a3-8b3e-43c5-9961-4b5e48e32a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the randomized search\n",
    "rnd_search_cv1 = RandomizedSearchCV(\n",
    "    estimator=regressor_search1,\n",
    "    param_distributions=parameters1,\n",
    "    n_iter=30,\n",
    "    cv=3,\n",
    "    verbose=1, \n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_absolute_error',  \n",
    "    random_state=69\n",
    ")\n",
    "\n",
    "# perform the random search on our training data\n",
    "rnd_search_cv1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45b8a04-583d-4677-8969-372d4c524a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best params:\", rnd_search_cv1.best_params_)\n",
    "print(\"Best score (neg MAE):\", rnd_search_cv1.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d9c16-783a-4c2f-b541-d7a176b9e258",
   "metadata": {},
   "source": [
    "Thing that is annoying about KerasRegressor is that you have to retrain to get history so you can plot training loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ccb6a7-84e3-491c-bf83-5e6bb738d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "best_params = rnd_search_cv1.best_params_\n",
    "\n",
    "def create_best_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(x_train.shape[1],)))\n",
    "    model.add(Dense(best_params['neurons1'], activation=best_params['activation1'],\n",
    "                    kernel_initializer='normal'))\n",
    "    model.add(Dense(best_params['neurons2'], activation=best_params['activation2'],\n",
    "                    kernel_initializer='normal'))\n",
    "    model.add(Dense(best_params['neurons3'], activation=best_params['activation3'],\n",
    "                    kernel_initializer='normal'))\n",
    "    model.add(Dense(1, activation='linear', kernel_initializer='normal'))\n",
    "   \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='huber', optimizer=optimizer,\n",
    "                  metrics=['mae', 'RootMeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "final_model = create_best_model()\n",
    "history = final_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=75,\n",
    "    batch_size=best_params.get('batch_size', 32),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred = final_model.predict(x_test).flatten()\n",
    "y_true = y_test.values.flatten()\n",
    "mae = metrics.mean_absolute_error(y_true, y_pred)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_true, y_pred))\n",
    "r2 = metrics.r2_score(y_true, y_pred)\n",
    "pearson, _ = pearsonr(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2.5)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2.5)\n",
    "plt.title('Learning Curves – Final Model', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Huber Loss')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(y_true, y_pred, alpha=0.7, edgecolor='k', s=80)\n",
    "minmax = [min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())]\n",
    "plt.plot(minmax, minmax, 'r--', lw=2)\n",
    "plt.xlabel('True Enorm (eV)')\n",
    "plt.ylabel('Predicted Enorm (eV)')\n",
    "plt.title(f'Parity Plot (R² = {r2:.4f}, MAE = {mae:.4f} eV)', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "X = x_train.to_numpy() if hasattr(x_train, 'to_numpy') else np.asarray(x_train)\n",
    "y_raw = y_train.to_numpy().flatten() if hasattr(y_train, 'to_numpy') else np.asarray(y_train).flatten()\n",
    "y = y_raw.reshape(-1, 1)\n",
    "\n",
    "fold = 5\n",
    "kf_val = KFold(n_splits=fold, shuffle=True, random_state=10)\n",
    "\n",
    "r2_scores = []\n",
    "mae_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "fold_no = 1\n",
    "for train_idx, val_idx in kf_val.split(X):\n",
    "    print(f\"Fold {fold_no}/{fold}\")\n",
    "\n",
    "    X_train_fold = X[train_idx]\n",
    "    X_val_fold = X[val_idx]\n",
    "    y_train_fold = y[train_idx]\n",
    "    y_val_fold = y[val_idx]\n",
    "\n",
    "    model = create_best_model()\n",
    "\n",
    "    model.fit(X_train_fold, y_train_fold,\n",
    "              epochs=75,\n",
    "              batch_size=best_params.get('batch_size', 32),\n",
    "              verbose=0)\n",
    "\n",
    "    y_pred_fold = model.predict(X_val_fold).flatten()\n",
    "    y_true_fold = y_val_fold.flatten()\n",
    "\n",
    "    r2_fold = r2_score(y_true_fold, y_pred_fold)\n",
    "    mae_fold = mean_absolute_error(y_true_fold, y_pred_fold)\n",
    "    rmse_fold = np.sqrt(mean_squared_error(y_true_fold, y_pred_fold))\n",
    "\n",
    "    r2_scores.append(r2_fold)\n",
    "    mae_scores.append(mae_fold)\n",
    "    rmse_scores.append(rmse_fold)\n",
    "\n",
    "    print(f\"  Fold {fold_no} - R²: {r2_fold:.4f}, MAE: {mae_fold:.4f}, RMSE: {rmse_fold:.4f}\")\n",
    "    fold_no += 1\n",
    "\n",
    "print(\"\\n 5-Fold Cross-Validation Results for NN\")\n",
    "print(f\"Mean R²:  {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "print(f\"Mean MAE: {np.mean(mae_scores):.4f} ± {np.std(mae_scores):.4f}\")\n",
    "print(f\"Mean RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")\n",
    "\n",
    "final_model.save('final_migration_barrier_nn.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7035ae-c812-4f7c-965a-a8d00bf0adae",
   "metadata": {},
   "source": [
    "Let's try to build a random forest from decision trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dfc235-ac45-4f2a-9fc0-ca1efe86b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# #attempt 2 gives 0.821 R^2 at tree_depth 5\n",
    "# x_dt = map_df[['Site2_MeltingT', 'Site2_BCCenergy_pa', 'Site2_NdUnfilled',\n",
    "#                'Site2_Density', 'Site1_MendeleevNumber', 'valence_arithmetic_average',\n",
    "#                'Site1_Electronegativity','Site1_MendeleevNumber','Site1_CovalentRadii',\n",
    "#                'Site1_MiracleRadius', 'BCCenergy_pa_arithmetic_average']].copy().astype(float)\n",
    "\n",
    "\n",
    "#even more features, decision tree gave 0.813 R^2 \n",
    "x_dt = map_df[['Site2_MeltingT', 'Site2_BCCenergy_pa', 'Site2_NdUnfilled', \n",
    "                   'Site2_Density', 'Site1_MendeleevNumber', 'valence_arithmetic_average', 'IonicRadii_max_value',\n",
    "                   'Site1_Electronegativity','Site1_MendeleevNumber','Site1_CovalentRadii','Site1_MiracleRadius', 'BCCenergy_pa_arithmetic_average']].copy().astype(float)\n",
    "\n",
    "\n",
    "y_dt = map_df[['Enorm (eV)']].astype(float)\n",
    "\n",
    "x_traindt, x_testdt, y_traindt, y_testdt = train_test_split(x_dt, y_dt, test_size=0.2, random_state=69)\n",
    "\n",
    "DT = tree.DecisionTreeRegressor(max_depth=5)\n",
    "DT.fit(x_traindt, y_traindt.values.ravel())  # ravel to avoid warnings\n",
    "dt_trainpred = DT.predict(x_traindt)\n",
    "dt_testpred = DT.predict(x_testdt)\n",
    "\n",
    "train_r2 = r2_score(y_traindt, dt_trainpred)\n",
    "test_r2 = r2_score(y_testdt, dt_testpred)\n",
    "train_mae = mean_absolute_error(y_traindt, dt_trainpred)\n",
    "test_mae = mean_absolute_error(y_testdt, dt_testpred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_traindt, dt_trainpred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_testdt, dt_testpred))\n",
    "\n",
    "print(\"The training R² is\", train_r2)\n",
    "print(\"The training MAE is\", train_mae)\n",
    "print(\"The training RMSE is\", train_rmse, \"\\n\")\n",
    "print(\"The test R² is\", test_r2)\n",
    "print(\"The test MAE is\", test_mae)\n",
    "print(\"The test RMSE is\", test_rmse)\n",
    "\n",
    "# MSE vs depth plot\n",
    "train_mse_list = []\n",
    "test_mse_list = []\n",
    "trees = np.arange(1, 20, 1)\n",
    "for t in trees:\n",
    "    dt_model = tree.DecisionTreeRegressor(max_depth=t)\n",
    "    dt_model.fit(x_traindt, y_traindt.values.ravel())\n",
    "    train_pred = dt_model.predict(x_traindt)\n",
    "    test_pred = dt_model.predict(x_testdt)\n",
    "    train_mse = mean_squared_error(y_traindt, train_pred)\n",
    "    test_mse = mean_squared_error(y_testdt, test_pred)\n",
    "    train_mse_list.append(train_mse)\n",
    "    test_mse_list.append(test_mse)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(trees, train_mse_list, marker='o', label='Train MSE')\n",
    "plt.plot(trees, test_mse_list, marker='s', label='Test MSE')\n",
    "plt.legend()\n",
    "plt.xlabel('Max tree depth')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('MSE vs Max Tree Depth')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fold = 5\n",
    "kf_val = KFold(n_splits=fold, shuffle=True, random_state=10)\n",
    "dt_r2_scores = cross_val_score(DT, x_dt, y_dt.values.ravel(), cv=kf_val, scoring='r2')\n",
    "print(\"Cross-validation R² scores:\", dt_r2_scores)\n",
    "print(\"Mean CV R²:\", dt_r2_scores.mean())\n",
    "\n",
    "dt_mse_scores = cross_val_score(DT, x_dt, y_dt.values.ravel(), cv=kf_val, scoring='neg_mean_squared_error')\n",
    "print(\"Cross-validation neg MSE scores:\", dt_mse_scores)\n",
    "print(\"Mean CV MSE:\", -dt_mse_scores.mean())\n",
    "\n",
    "plt.figure(figsize=(8.5, 8.5))\n",
    "\n",
    "y_dt_min = y_dt.min().item()\n",
    "y_dt_max = y_dt.max().item()\n",
    "trainpred_min = dt_trainpred.min()\n",
    "trainpred_max = dt_trainpred.max()\n",
    "testpred_min = dt_testpred.min()\n",
    "testpred_max = dt_testpred.max()\n",
    "\n",
    "overall_min = min(y_dt_min, trainpred_min, testpred_min) - 0.05\n",
    "overall_max = max(y_dt_max, trainpred_max, testpred_max) + 0.05\n",
    "\n",
    "plt.scatter(y_traindt, dt_trainpred,\n",
    "            color='steelblue', alpha=0.6, s=60, label='Training data', edgecolor='k', linewidth=0.3)\n",
    "\n",
    "plt.scatter(y_testdt, dt_testpred,\n",
    "            color='crimson', alpha=0.85, s=80, label='Test data', edgecolor='k', linewidth=0.5)\n",
    "\n",
    "plt.plot([overall_min, overall_max], [overall_min, overall_max],\n",
    "         color='black', linestyle='--', linewidth=2, label='Regression line')\n",
    "\n",
    "plt.title(f'Decision Tree Regressor — Test R² at tree depth 5 = {test_r2:.3f}', fontsize=16, pad=20)\n",
    "plt.xlabel('Actual Enorm (eV)', fontsize=14)\n",
    "plt.ylabel('Predicted Enorm (eV)', fontsize=14)\n",
    "\n",
    "textstr = f'MAE = {test_mae:.3f} eV\\nRMSE = {test_rmse:.3f} eV\\nPoints = {len(y_testdt)}'\n",
    "plt.text(0.04, 0.96, textstr, transform=plt.gca().transAxes, fontsize=11,\n",
    "         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.9))\n",
    "\n",
    "plt.legend(loc='lower right', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2bd036-59ce-4c38-bcb9-451b06897f4f",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "This model I build is a supervised machine learning regression model. Unlike what we learnt in class(using neural network and random forest for classification), I used both the machine learning methods for regression since, instead of classification, I need to predict a line of float numbers. The reason why I chose neural network and decision tree because it was obvious that the relationship between the features and Enorm isn't linear, and linear regression wouldn't be able to map the relationship well. Neural networks and decision trees are widely used for non-linear relationship prediction in data. <br>\n",
    "\n",
    "To reiterate, we are building a model that could predict the Enorm migration barrier activation energy using different related features I can find in the dataset. <br>\n",
    "\n",
    "On the discussion before I start training a model, I explained my train of thought on how I picked the 4 features. The 4 features are: <br>\n",
    "\n",
    "site2meltingT, site2bccenergy, site2ndunfilled, site2density <br>\n",
    "\n",
    "The key performance metric I used for my regression models is the $R^2$ score between predicted Enorm and actual Enorm value, as it measures the proportion of variance in actual Enorm from predicted Enorm. Since it is a supervised machine learning, we have ground truth predicted value, and how different our predicted values to the ground truth will be a good metric to determine the model performance. I also used mean absolute error and mean-squared error to check the error percentage (accuracy is not applicable as a metric in a regression model).\n",
    "                        \n",
    "The first $R^2$ from the predicted vs actual was an awful 0.435 $R^2$ while using just the 4 features for my neural network. Not only that I used the hyperparameter tuning we learned in class to pick the optimal activation functions and neurons for the layers, while using 4 features, even if I raised my training epochs to 200, the validation loss and training loss comes to a plateau after 50 epochs. Since shape of validation loss curve didn't spike and k-fold cross validation average $R^2$ was consistent (awfully low, mean 0.321) with the $R^2$ score, I believe that the model didn't overfit, but instead underfit.<br> \n",
    "\n",
    "However, my decision tree performed really well and had test $R^2$ of 0.8 and mean cross-validation $R^2$ of 0.758. <br>\n",
    "\n",
    "\n",
    "Then, I thought I could add more features and check whether my mode will perform better - and it did which is expected. I introduced more features that are related to bonding energy of the atoms:\n",
    "\n",
    "| Category                          | Feature                              |\n",
    "|-----------------------------------|--------------------------------------|\n",
    "| Site 2 Properties                 | `Site2_MeltingT`                     |\n",
    "|                                   | `Site2_BCCenergy_pa`                 |\n",
    "|                                   | `Site2_NdUnfilled`                   |\n",
    "|                                   | `Site2_Density`                      |\n",
    "| Site 1 Properties                 | `Site1_MendeleevNumber`              |\n",
    "|                                   | `Site1_Electronegativity`            |\n",
    "|                                   | `Site1_CovalentRadii`                |\n",
    "|                                   | `Site1_MiracleRadius`                |\n",
    "| Mixed / Average Properties        | `valence_arithmetic_average`         |\n",
    "|                                   | `BCCenergy_pa_arithmetic_average`    |\n",
    "\n",
    "\n",
    "and the predicted vs actual $R^2$ became 0.809, with average cross validation $R^2$ of 0.7279 ± 0.0766. I believe adding more related features, such as Ionicradii_max_value will also help, but adding too many features could cause overfitting. \n",
    "\n",
    "For my decision tree, test MSE plateaued at tree depth 5 for all features, but it always shown consistent test $R^2$ of ~0.8 and average cross validation $R^2$ of ~0.75 across all 3 attempts with different number of features, despite NN performing badly with fewer features. <br>\n",
    "\n",
    "To answer why random forest outperforms NN at fewer features, (from the internet), it is because NN requires a lot of data and learn non-linearities without underfitting, and our dataset has a small sample size. Our dataset is also tabulated, where random forest excels compared to NN, where NN usually does well on voices/images. Random forest also works well and robustly on small noisy datasets as it handles low data volumes robustly by acceraging predictions and reducing variance, where NN can plateau early (underfit) working on small, noisy datasets. However, NN is really good at working with huge, complex datasets with hierarchy where random forest might not excel. <br>\n",
    "\n",
    "Bottom line is, using the correct model based on the nature of the dataset you have. Neural network is suitable for large, complex datasets, and random forest is more suitable for small, noisy data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88987db-26d1-4e96-89c8-8192323b07f4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aaa4ef-c006-422c-a48f-fb306fc3d94b",
   "metadata": {},
   "source": [
    "With domain-knowledge-guided feature selection, we hypothesized that it is possible to predict Enorm level if given data of properties related to bonding strength, such as melting point, orbital filling status, radii of solute atom, electronegativity of the solute atom and etc, since bonding energy is what will dictate how much energy is needed for an atom to 'escape'. <br>\n",
    "\n",
    "These properties, when compared to Enorm individually, weren't able to explain much of the variance of Enorm, but when used to predict Enorm together, they were able to explain majority (80%) of the variance in Enorm. Although not absolute or publication-level accuracy, this supervised machine learning model validates our hypothesis as strong and reliable. Besides that, this project was a good example to prove how random forest outperforms neural network in small, noisier sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e30cc27-e1c4-4f12-b5a4-b895f4f94865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
